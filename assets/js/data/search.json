[ { "title": "The importance of Open Source Software: Terraform and OpenTofu", "url": "/posts/importance-of-opensource/", "categories": "blog", "tags": "", "date": "2023-10-28 10:00:00 +0100", "snippet": "IntroductionOpen source software has played a pivotal role in shaping the modern technological landscape. It has enabled developers, businesses, and communities to collaborate and build robust ecosystems. However, recent developments in the open-source world have sparked a significant debate and raised concerns about the future of these projects. In August 2023, Terraform, a popular infrastructure as code (IaC) tool, transitioned from an open-source license to the Business Source License (BUSL). This sudden change has far-reaching implications not only for Terraform but for the entire open-source community. In response, a group of concerned individuals created OpenTofu, a fork of Terraform, which is now maintained by the Linux Foundation.The significance of TerraformTerraform, developed by HashiCorp, is a critical tool for managing infrastructure as code. It has been a mainstay in the tech industry for nearly a decade, fostering a thriving community of users, contributors, vendors, and open-source modules. Its open-source nature allowed for widespread adoption and innovation. However, the sudden shift to the BUSL license has raised a red flag in the open-source world.The threat of the Business Source License (BUSL)The BUSL license, in essence, is not open source. It introduces ambiguities that leave many businesses, developers, and users uncertain about their legal standing when utilizing Terraform. The uncertainty stems from the fact that the BUSL and its accompanying use grant lack clarity. What exactly constitutes “competitive” with HashiCorp’s offerings? What if interpretations change over time? These uncertainties place everything dependent on Terraform on shaky ground, potentially stifling its ecosystem.Erosion of the Open-Source ecosystemThe move to the BUSL license threatens to undermine the very foundation of Terraform’s thriving open-source ecosystem. Developers and companies that once relied on Terraform may reconsider their choices in favor of tools with genuinely open-source licenses. This shift can result in the stagnation and eventual disappearance of existing Terraform codebases, independent tooling, and community support. The open-source community built around Terraform risks fragmentation and decline, potentially leaving users stranded.A wider implicationThe events surrounding Terraform’s licensing change carry implications beyond this specific tool. Any sudden transition from open source to a restrictive license model can cast a shadow of doubt over the open-source world as a whole. This serves as a warning to every company and developer considering open-source projects. They now must think twice about adoption and investment, as the threat of a future license change hangs over the entire open-source landscape.The birth of OpenTofuIn response to these challenges, a group of individuals took action to safeguard Terraform and its open-source values. They asked HashiCorp to revert Terraform to an open-source license, but with no response, they decided to create a fork, which they named OpenTofu. As already mentioned above, this fork is now maintained by the Linux Foundation.Why OpenTofu mattersOpenTofu’s creation in the Linux Foundation signifies a commitment to preserving the open-source ethos. I encourage all of you to read the OpenTofu Manifesto to have a better understanding of why this is so important.My 2 centsThe Terraform licensing shift and the emergence of OpenTofu exemplify the fundamental importance of open-source software in our interconnected world. These events serve as a reminder that open-source principles are essential for creating a strong, reliable foundation for our technological infrastructure. They underline the need for transparency, collaboration, and community-driven development in open-source projects. OpenTofu’s commitment to these principles reinforces the idea that open-source software should be embraced, cherished, and protected to ensure a vibrant and innovative future for technology." }, { "title": "In the Abyss of Kubernetes: A SysAdmin&#39;s Descent into Cosmic Horror", "url": "/posts/kubernetes-abyss/", "categories": "blog, micro-novels", "tags": "stories, AI-generated, kubernetes", "date": "2023-06-10 12:00:00 +0100", "snippet": "It is with trepidation and a trembling heart that I recount the tale of a weary SysAdmin, delving into the depths of Kubernetes, where sanity wanes and eldritch horrors lurk. Beware, dear reader, for this is not a tale for the faint of heart, but one that reveals the cosmic horrors that await those who dare to approach this arcane technology.Our protagonist, a humble SysAdmin, was called upon to tame the chaos of a sprawling infrastructure. Little did they know that the path they chose would lead them to the abyss of Kubernetes, where ancient and unfathomable beings lie in wait. With trembling hands, they embarked on a journey that would test the limits of their understanding and sanity.As the SysAdmin delved deeper into the abyss, they encountered a dark and enigmatic force known as the “kubectl”. Its arcane commands whispered secrets and madness into their ears, leaving them questioning the very fabric of reality. The sheer complexity of Kubernetes seemed like an eldritch puzzle, taunting them with its cryptic terminology and intricate architecture.The cluster itself became a labyrinth of horrors, where nodes spawned and died like cosmic entities in an ever-shifting universe. The SysAdmin, armed with their knowledge, attempted to bring order to the chaos, but each step they took seemed to lead them deeper into a maze of unfathomable complexity. The twisted tendrils of pods and services ensnared them, threatening to consume their very soul.But it was the eternal dance of scaling and orchestration that truly pushed the SysAdmin to the brink of madness. As they struggled to balance the delicate equilibrium of resources, they found themselves trapped in an eldritch nightmare, tormented by the shifting demands of applications. The line between reality and abstraction blurred, leaving them questioning the nature of their existence.In their darkest moments, the SysAdmin encountered the horrors of debugging. The mysterious logs whispered fragments of forbidden knowledge, revealing cryptic error messages that mocked their feeble attempts at comprehension. Each failure felt like a cosmic force conspiring against them, plunging them further into a realm of uncertainty and despair.Yet, amidst the chaos and despair, the SysAdmin glimpsed moments of transcendence. They witnessed the true power of Kubernetes, as it orchestrated an army of containers with an otherworldly precision. The ability to scale applications effortlessly and achieve fault-tolerant deployments seemed like a glimpse into a cosmic design beyond mortal comprehension.As the SysAdmin reflected upon their descent into the abyss of Kubernetes, they realized that their struggle was not in vain. They had ventured into the realms of cosmic horror, facing the eldritch beings of abstraction and complexity. Though scarred and forever changed, they emerged with newfound wisdom and resilience.But alas, dear reader, the story does not end with triumph. As the SysAdmin continued their battle, the veil between reason and madness grew thin. They descended further into the depths of Kubernetes, their mind consumed by the eldritch nightmares that lurked within. Madness, like a relentless tide, swept over them, leaving nothing but a shattered and deranged soul in its wake.So, dear reader, let the SysAdmin’s tragic tale be a cautionary one. Approach Kubernetes with trembling steps and a mind prepared for the abyss that awaits. For in the darkest depths of Kubernetes, where complexity reigns supreme, the SysAdmin dances on the precipice of madness. May you be guided by the flickering light of reason as you navigate this treacherous realm, haunted by the specters of cosmic horror." }, { "title": "Embracing the Chaotic Beauty of Self-Hosting: A Love Story", "url": "/posts/embracing-the-chaotic-beauty-of-self-hosting/", "categories": "blog, micro-novels", "tags": "stories, AI-generated, self-hosted", "date": "2023-06-05 20:00:00 +0100", "snippet": "Once upon a time, in a land far, far away, there lived a curious internet adventurer named A1ic3. A1ic3 loved exploring the vast digital wilderness, but she was tired of relying on third-party platforms to host her website. She yearned for independence, control, and a dash of chaos. Little did she know that her quest for self-hosting would lead her down a path filled with hilarious mishaps and unexpected moments of beauty.Our tale begins with A1ic3’s decision to venture into the wild world of self-hosting. Armed with determination and a beginner’s guide to web development, she set out to create her very own virtual castle. But as any explorer knows, the path to self-hosting is paved with countless absurd challenges.A1ic3 quickly discovered that setting up her own server was like assembling an IKEA bookshelf without instructions—maddeningly hilarious. She found herself knee-deep in cables, surrounded by a tangled mess of wires that seemed to breed like rabbits when she wasn’t looking. Who knew that the physical manifestation of internet connectivity could resemble a nest of particularly disgruntled snakes?But amidst the chaos, A1ic3 found beauty. She marveled at the power of learning and the satisfaction of overcoming hurdles that seemed insurmountable at first. Each successfully connected wire felt like a tiny victory, a step closer to the enchanting realm of self-hosting.Of course, A1ic3’s journey wouldn’t be complete without the occasional dance with disaster. She experienced more downtime than a sleep-deprived toddler, and her website seemed to disappear into the void at the most inconvenient moments. Oh, the joy of debugging late into the night, armed with only caffeine and the unwavering belief that she could conquer any error message that dared cross her path!But even in the darkest moments, A1ic3’s sense of humor shone brightly. She learned to laugh at the absurdity of it all, embracing the unpredictable nature of self-hosting. Who needs a horror movie when you can experience the nail-biting suspense of watching your website crash and burn with a single misplaced semicolon?And then, one fateful day, it happened. A1ic3’s website stood proudly, hosted on her very own server. The sense of accomplishment overwhelmed her, and she felt like she had conquered Mount Everest armed with nothing but a rusty spoon. Her website might not have been the fastest or the most stable, but it was hers—a testament to her resilience, determination, and, well, questionable life choices.As A1ic3 reflected on her tumultuous journey, she realized that self-hosting wasn’t just about the technical aspects; it was a mindset. It was about embracing imperfection and finding joy in the unpredictable. It was about taking ownership of her virtual presence, even if it came with the occasional 404-page surprise party.So, dear reader, if you ever find yourself yearning for a touch of chaos and a heaping dose of control, consider embarking on the adventure of self-hosting. Embrace the beauty in the unexpected, laugh at the absurdity, and remember that every tangled wire and cryptic error message is a stepping stone on the path to digital independence.And who knows? Perhaps you’ll find that the true beauty of self-hosting lies not in perfection, but in the joy of conquering chaos, one tangled wire at a time." }, { "title": "A year later", "url": "/posts/homelab-blog-one-year/", "categories": "blog, stories", "tags": "homelab, blog, anniversary", "date": "2022-04-15 18:00:00 +0100", "snippet": "Time flies when you are having fun! I spent the last year learning, tinkering, testing, getting mad… but I’m still here, ready to do it again and again! Doing all these things while in a full time job is not easy. Sometimes when you come back from work you just want to restand chill, and it’s difficult to find the strength to carry on. So, it was challenging, but also fun and rewarding. Let’s wrap it up!New setupI changed the setup after a couple of months, and this is what I have done: Bought a new machine and migrated all my Docker containers there Get rid of Proxmox and installed ESXi instead Set up few VMs for testing and practice with Ansible Bought a HP Proliant Microserver N40L and installed OpenMediaVault on itNow I have a bigger playground, and I feel more in control (mainly because I know I have backups on my NAS). Recently I destroyed all the VMs in ESXi and set up a 5 nodes Kubernetes cluster using K3s.The biggest winMaybe for some people this is not considered a big win, but it is for me. I coded a simple Telegram Bot in Python and created a container image of it. I then created a deployment for this botin my K3s cluster. For the final touch, I set up ArgoCD to monitor the repository where the deployment manifest is,so if I want to change something (e.g. put the bot in maintenance mode) I can just edit the manifest and ArgoCD will dothe rest for me.The biggest failI guess this is a common mistake… One of the first thing I have tried with Ansible was trying to configure sshd_config. The idea was simple, just changeSSH port, disable root login, and set up AllowUsers. I know you already guessed where this is going… So, I set the template out, set the AllowUsers with a user I had on a different machine and run the playbook.Everything went fine without any errors, but I soon realised I messed up the user and locked myself out of that machine.Luckily I had physical access to that machine, so I managed to get in and fix my mistake. This is something that will nothappen again.A piece of adviceI really enjoy administering my home lab, trying to improve things, implement automation when I can… it’s so much fun!As I said earlier, sometimes could be challenging if you are in a full time job. You are constantly thinking how to fixthat issue, or how to automate this or that, what technology you need to learn, and so on… If you are in the same situation and you are struggling to keep it up, keep this in mind: it’s ok to take a break." }, { "title": "Documentation, Documentation, Documentation!", "url": "/posts/bookstack/", "categories": "homelab, self-hosted", "tags": "bookstack, homelab, self-hosted, docker, documentation", "date": "2022-03-26 10:00:00 +0000", "snippet": "Recently someone asked on Reddit: “What’s your number one recommended service?”Everyone could answer this question differently, but my answer was: “Bookstack. Start documentation as soon as possible!”.This is a piece of advice I would give to myself at the very beginning. Of course, working on projects like setting up a mediaserver could be fun, and also you can use it almost immediately. Documentation is something that most of us (me included!) tendto postpone with a plethora of excuses. I thought documentation is useful only when working on big projects, not for ahomelab, but you can always find something to write in the documentation. You can write your configurations, detailing the stepsto set up your homelab, issues you encountered and how you fixed or dealt with them… the list goes on and on.Nobody taught me how to write documentation, and I am sure it’s not great, but I am doing my best, and I am keeping track ofwhat I am doing in my homelab.So, as you probably guessed, I use Bookstack for my documentation. Some people may prefera Wiki-style application, but I like the idea of having Books, Chapters and Pages, and it’s very easy to use.I don’t think I need to say that I used docker-compose to set it up, but if you fancy something different you can read thedocumentation.mkdir bookstackcd bookstacktouch docker-compose.ymlnano docker-compose.ymldocker-compose.yml---version: &quot;2&quot;services: bookstack: image: lscr.io/linuxserver/bookstack container_name: bookstack environment: - PUID=1000 - PGID=1000 - APP_URL= - DB_HOST=bookstack_db - DB_USER=bookstack - DB_PASS=&amp;lt;yourdbpass&amp;gt; - DB_DATABASE=bookstackapp volumes: - /path/to/data:/config ports: - 6875:80 restart: unless-stopped depends_on: - bookstack_db bookstack_db: image: lscr.io/linuxserver/mariadb container_name: bookstack_db environment: - PUID=1000 - PGID=1000 - MYSQL_ROOT_PASSWORD=&amp;lt;yourdbpass&amp;gt; - TZ=Europe/London - MYSQL_DATABASE=bookstackapp - MYSQL_USER=bookstack - MYSQL_PASSWORD=&amp;lt;yourdbpass&amp;gt; volumes: - /path/to/data:/config restart: unless-stoppedSetting up the environment variables is pretty straightforward, just pay attention to match the database passwords.sudo docker-compose up -dNow, if you didn’t read the documentation (naughty!), you can login using admin@admin.com and password of password.Go and write your own documentation!" }, { "title": "My way to du-it.sh", "url": "/posts/du-it/", "categories": "automation, scripting", "tags": "du-it, docker, automation, bash, scripting", "date": "2022-03-18 10:00:00 +0000", "snippet": "The other day was maintenance day for my home lab, which mainly means updating docker containers. This time I decidedto take a different approach and, rather than using Watchtower, I openedmy trusted IDE and created a Bash Script.The reason why I decided to go for a Bash Script is because I had an issue with an update using Watchtower, which basically wipedout the container’s data (luckily that container was in staging). I am sure the issue was not related to Watchtower itself,but it was a bit annoying going through the setup process again.NOTE: Watchtower gives you the option to exclude some containers. Take a look at thisfor more details.I wanted something very simple (always KISS): while executing the script, Ipass the containers’ name as arguments, than the script asks me if I want to Delete or Update them (that’s why DU-it).IMPORTANT: Keep in mind that this script is based on my lab, where I mainly use docker-compose and every folder is named with thesame name of the container, therefore it may not work in your environment.DeleteThis option stops and removes the container, it also deletes the image used by that container.The script will check if the argument provided is a valid match for a container’s name and, if it is, it creates a container_name.tmpfile which contains the image used by that container; than, stops and remove that container and cat out the .tmp file toknow what image needs to be deleted. Once done, all the .tmp files will be deleted.UpdateThis option goes through all the steps of the previous option, but also goes into the container’s directory and executesa docker-compose up -d so it will pull down the new and updated image.As final touch, I gave each option a colour, so I could easily recognise what’s going on if I get distracted doing somethingelse.I am sure it can be written in a better way, but at the end of the day it works just fine to me. Also, there’s always room for futureimprovements.You can see the script on my GitHub here." }, { "title": "DIUN: Docker Image Update Notifier", "url": "/posts/diun/", "categories": "homelab, self-hosted", "tags": "diun, homelab, self-hosted, docker, alerting", "date": "2022-03-11 16:50:00 +0000", "snippet": "I believe we all agree that automate updates (in general) might not be a good idea. If we are talking about Docker images, we need tomonitor the registries to see if a new release is out, than read the release notes and decide whether to update to the new version or not.Well, what if I tell you that we can at least skip the monitoring part and get a notification to our phone when a new version of an image is released?Maybe it doesn’t seem much, but if you like to manage your time as efficiently as possible, like I do, you can see the benefit of it.Let me introduce you Diun! Long story short, Diun receives a notification when a Docker imageis updated on a Docker registry.As you can see in the documentation, you can run Diun in different ways, but I chose docker-compose.What a surprise!mkdir diuncd diuntouch docker-compose.ymlnano docker-compose.ymldocker-compose.yml---version: &quot;3.5&quot;services: diun: image: crazymax/diun:latest container_name: diun command: serve volumes: - &quot;./data:/data&quot; - &quot;/var/run/docker.sock:/var/run/docker.sock&quot; environment: - &quot;TZ=Europe/London&quot; - &quot;LOG_LEVEL=info&quot; - &quot;LOG_JSON=false&quot; - &quot;DIUN_WATCH_WORKERS=20&quot; - &quot;DIUN_WATCH_SCHEDULE=0 0 09 * * *&quot; - &quot;DIUN_PROVIDERS_DOCKER=true&quot; labels: - &quot;diun.enable=true&quot; restart: unless-stoppedI changed the schedule to make it run every day at 09:00 (take a look at CRON Expressions Formatif you need). But how do we get notified?Diun supports many services, but for this kind of projects I usually choose Telegram.Set up a Telegram Bot which will send the notifications and, once done, add some environment variables in the docker-compose file:- DIUN_NOTIF_TELEGRAM_TOKEN=- DIUN_NOTIF_TELEGRAM_CHATIDS= #can be more than one (comma separated)- DIUN_NOTIF_TELEGRAM_TEMPLATEBODY=You can reference the documentation to find more details, including thedefault templateBody which you can modify according to your needs. Highly recommended if you run Diun on multiple machines.Now we can finally spin it up:sudo docker-compose up -dA quick note: to make sure everything is working fine, run a test using:sudo docker-compose exec diun diun notif testYou should receive a notification on your phone from your new Diun Telegram Bot." }, { "title": "Jellyfin, a versatile media server", "url": "/posts/jellyfin/", "categories": "homelab, self-hosted", "tags": "jellyfin, homelab, self-hosted, docker, media", "date": "2022-02-12 17:50:00 +0000", "snippet": "Are you looking for another homelab project? Well, why don’t set up a media server?A media server is a nice thing to set up in a homelab, also because it’s something that anyone in the house can use.There are a ton of options out there, but I decided to go for Jellyfin simply because it’s versatile.If you would like to see a comparison between different media servers, take a look at this on GitHub.I know what you are thinking and the answer is “yes, we are going to use docker-compose”. Also, we are going to use the Docker image fromlinuxserver.io.Without further ado, let’s set it up!I’m going to set up some shared folders in my NAS (you don’t have to), so I can have a good amount of space for my media, and then…mkdir jellyfincd jellyfintouch docker-compose.ymlnano docker-compose.ymldocker-compose.yml---version: &quot;2.1&quot;services: jellyfin: image: lscr.io/linuxserver/jellyfin container_name: jellyfin environment: - PUID=1000 - PGID=1000 - TZ=Europe/London volumes: - /path/to/library:/config - /path/to/tvseries:/data/tvshows - /path/to/movies:/data/movies ports: - 8096:8096 restart: unless-stoppedAfter writing, saving and closing the file, execute the usual…sudo docker-compose up -dWait until the container is up and running. Now go to the ip address:port where Jellyfin lives and start the final set up.Jellyfin has mobile and Smart TV applications, so you can access it even from your couch or on the go.Warning: Don’t let the CPU dieSomething to consider when setting up a media server is that without a graphics card, the CPU will do all the heavy lifting.This may not be a good thing on older hardware, and because (surprise, surprise) I have old hardware, I decided to buy a graphics card.Remember, you don’t need the latest and greatest hardware, just something to do the job smoothly and leave the CPU in peace.Just to give you an example, I bought a GeForce GT610, and it works amazingly well. So, if you are in the same situation,do yourself a favor and don’t let your CPU die." }, { "title": "Homelab: the beginning", "url": "/posts/homelab-blog-beginning/", "categories": "blog, stories", "tags": "homelab, blog, beginning", "date": "2022-01-13 10:00:00 +0000", "snippet": "A penguin made me do itIn January 2021 I had a talk with myself and we agreed that I needed a change. As I moved to a new city, I thought it was the perfect time to do something new, and so I did. I spent a couple of months exploringwhat was beyond my comfort zone and, eventually, I found myself thinking about a career change. There was a penguin whispering in my ear…T: You always liked Linux! Why don’t you go and find a job where you can do some Linux-Fu?M: How? There are so many technologies out there that I never used!T: Build a homelab!That penguin was right. I should have given the homelab idea a go, so in April 2021 I bought an old PC that I would use for my homelab, and my adventure began.Since I was absolutely new to the game, I spent quite some time gathering information about building and managing a homelab.Videos, blog posts, various documentation… I thought I had to spin up some VMs and start playing,but how? There were so many options, and I started feeling overwhelmed. That’s the moment when you have to keep going, otherwise you end up doing nothing.So, eventually I bumped into what would become my first hypervisor: ProxmoxVE. I installed Proxmox on my new machine, and I started the studying process.The revelationAfter downloading the Ubuntu Server ISO, it was time to spin up my first virtual machine in my homelab.Fortunately Proxmox is pretty easy to use, so creating a VM is a smooth experience, but once it was done, I felt lost.I didn’t know what to do, what to install, what to learn. I was looking for ideas on the web for days, until I found something calledPi-hole. I thought it would be cool to start with something like this, because it can also serve my home network.In that particular moment I realised why I was having troubles finding ideas! I was so focused on this concept of the homelab as a learning toolthat I forgot it doesn’t have just that purpose. That was a game changer.I could learn new things while actually implementing them in a real life situation.Since that moment, looking for new projects was so easy and fun. I couldn’t wait to start something new, implementinga new feature, or even just opening up the shell and make sure everything was running fine.VMs are good, but containers are betterDuring my journey, I spun up few VMs, and soon I ran out of resources (mainly RAM) so I was looking for a solution.The solution was Docker. I’m not going into details, so, if you don’t know what containerisation is,take a look at this.Because I wanted to learn as much as I could, I installed Docker in one of my VM and started messing around.I remember that at first, it was a weird experience, most likely because I was still learning the basics, but the more confident I became,the more containers I ended up spinning. Long story short, every application running in a VM was then moved in a Docker Container.It helped me save a lot of resources and, at the same time, it set my mindset to: if I can run it in a container, I will!A new beginningAfter a couple of months, my homelab adventure with Proxmox, few VMs and Docker, came to its end.Sad? Not really, because I was hungry for improvement! I decided to rebuild my homelab on top of a different hypervisor.Don’t get me wrong, I didn’t have any issue with Proxmox, I just wanted something more known in the industry: VMware ESXi.Once installed and ready to go, I created a VM to serve me with Docker, set up all the containers I needed and start anotherchapter of this adventure.I was so excited, motivated, and, more important, I had a plan!But this is another story…" }, { "title": "WireGuard, VPN made easy", "url": "/posts/wireguard-vpn/", "categories": "homelab, self-hosted", "tags": "wireguard, homelab, self-hosted, docker, vpn", "date": "2022-01-06 10:00:00 +0000", "snippet": "Rather than expose some services in your homelab to the wild public internet, wouldn’t it be better if you can access them safely from anywhere you are? Maybe from your phone?Well, I like the idea, and I set up a VPN to accomplish this mission.If you don’t know what a VPN is, take a look at this.I am not going to lie, networking is not my strongest point, but WireGuard is so easy to set up that anyone can do it!I am going to use a Docker image from linuxserver.io, an amazing place to find Docker images.Let’s move to the terminal.mkdir wireguardcd wireguardtouch docker-compose.ymlnano docker-compose.ymldocker-compose.yml---version: &quot;2.1&quot;services: wireguard: image: lscr.io/linuxserver/wireguard container_name: wireguard cap_add: - NET_ADMIN - SYS_MODULE environment: - PUID=1000 - PGID=1000 - TZ=Europe/London - SERVERURL=wireguard.domain.com #optional - SERVERPORT=51820 #optional - PEERS=1 #optional - PEERDNS=auto #optional - INTERNAL_SUBNET=10.13.13.0 #optional - ALLOWEDIPS=0.0.0.0/0 #optional volumes: - /path/to/appdata/config:/config - /lib/modules:/lib/modules ports: - 51820:51820/udp sysctls: - net.ipv4.conf.all.src_valid_mark=1 restart: unless-stoppedAll environment variables are pretty much self-explanatory, which it helps in the setup process. Edit the variables as you likeand spin this container up!sudo docker-compose up -dOnce the container is up and running, take a look at the logs.sudo docker-compose logsHere you should find a QR code you can use with the WireGuard app on your phone. Just scan the code and your phone is going toget the configuration automatically.Now you just need to go to your router setting, forward port 51820/udp of WireGuard and you are good to go.It was easy, wasn’t it?Of course, there is a lot more than this. Just remember: documentation is your friend!" }, { "title": "Lychee, not the fruit", "url": "/posts/lychee-photos/", "categories": "homelab, self-hosted", "tags": "lychee, homelab, self-hosted, docker, photo-management-system", "date": "2021-12-16 13:00:00 +0000", "snippet": "Do you like lychees? I bet you will like this one!Few weeks ago, I was looking for a self-hosted solution where I can store all my travel pictures and I eventually bumped into Lychee.Lychee is a free, simple and secure photo-management-system which can be run in a Docker container. Which is basically whatI was looking for! So I decided to try it out and… I loved it!That’s how I have done it. First, I needed a big room for my pictures. A shared folder in my NAS would do the job.I needed two more folders: a database folder (yes, we need a database) and a config folder.Once created these folders, it was time to create the containers.mkdir lycheecd lycheetouch docker-composenano docker-composedocker-compose.yml---version: &#39;3&#39;services: lychee_db: container_name: lychee_db image: mariadb:10 environment: - MYSQL_ROOT_PASSWORD=YourSuperSecretRootPassword - MYSQL_DATABASE=lychee - MYSQL_USER=YourUsername - MYSQL_PASSWORD=YourSuperSecretPassword expose: - 3306 volumes: - /path/to/db:/db networks: - lychee restart: unless-stopped lychee: image: lycheeorg/lychee container_name: lychee ports: - 90:80 volumes: - /path/to/config:/conf - /path/to/uploads:/uploads networks: - lychee environment: - PUID=1000 - PGID=1000 - PHP_TZ=Europe/London - DB_CONNECTION=mysql - DB_HOST=lychee_db - DB_PORT=3306 - DB_DATABASE=lychee - DB_USERNAME=YourUsername - DB_PASSWORD=YourSuperSecretPassword # has to match the above MYSQL_PASSWORD - STARTUP_DELAY=30 restart: unless-stopped depends_on: - lychee_dbnetworks: lychee:volumes: mysql:After adding all variables and volumes, just write, save the changes and then:sudo docker-compose up -dOnce the containers were up and running, I opened up a web browser and headed to the ip address:port where Lychee lives, set up usernameand password, and ta-dah! I now have my self-hosted photo-management-system.I can upload, download, share, get details of my pictures, implement 2FA… It’s simply amazing!I hope you like it too.If you would like to personalise this service even more, take a look at the Lychee Documentationand edit the docker-compose.yml accordingly." }, { "title": "Watchtower and Telegram notifications", "url": "/posts/watchtower-telegram/", "categories": "automation, self-hosted", "tags": "watchtower, homelab, self-hosted, docker, alerting", "date": "2021-12-11 10:00:00 +0000", "snippet": "Updates… It could be a tedious task, but it has to be done.If you run containers in your homelab, you can easily get to the point where manually update all of your container imagesbecomes a boring moment of your day. It would be nice if we can delegate this task to someone else and, maybe, receive a notificationonce all it’s done. Well, fortunately we can do this with Watchtower!IMPORTANT! Despite we can automate this boring task, my advice is to not do it and always update manually.Check the release notes first! Update always with the newest release might not be worth.To give it a go, we firstly need to create a Telegram Bot for Watchtower. We can ask the BotFatherto guide us in the process. When the bot creation is done, BotFather will give us our BOT_TOKEN. Keep it safe, we will need it later!Once we have our token, we need to get our CHAT_ID with GetIDsBot.It’s so exciting to create our personal bot, isn’t it?So, now it’s time to get our hands dirty into the terminal.I chose the docker-compose way. So…mkdir watchtowercd watchtowertouch docker-compose.ymlnano docker-compose.ymldocker-compose.yml---version: &#39;3.3&#39;services: watchtower: image: containrrr/watchtower container_name: watchtower restart: unless-stopped volumes: - &#39;/var/run/docker.sock:/var/run/docker.sock&#39; environment: - TZ=Europe/London - WATCHTOWER_LIFECYCLE_HOOKS=True - WATCHTOWER_NOTIFICATIONS=shoutrrr - WATCHTOWER_NOTIFICATION_URL=telegram://BOT_TOKEN@telegram/?channels=CHAT_ID - WATCHTOWER_DEBUG=true - WATCHTOWER_CLEANUP=true - WATCHTOWER_SCHEDULE=0 0 20 * * 0Watchtower uses shoutrrr to send notifications via different applications.Just replace BOT_TOKEN and CHAT_ID with your own ones, and it’s done!With this setup, Watchtower will do its magic every sunday at 20:00 (8:00pm), sends us notifications through our Telegram Botand delete the old images. Once we set all up, we can write and save the file.Now we can create our Watchtower container with:sudo docker-compose up -dIf everything goes well, we should receive a Telegram notification from our Watchtower Bot telling us when the first checkwill be performed.You can always refer to the Watchtower Documentation if you want to play around." } ]
